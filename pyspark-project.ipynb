{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark as ps\n",
    "import warnings\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # create SparkContext on all CPUs available: in my case I have 4 CPUs on my laptop\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 50 #can be set as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Handle_User=u'_____0__o______', Name=u'Mamba Out w/ 60 pts', Bio=u'My favorite NBA player scored 60 points in his final NBA game what did yours do?Lakers.Cubs.Panthers.Syracuse.Sabres.Barca.NYCFC. Periscope/Snapchat/IG: Jaroc93', Country=u'US', Location=u'Raleigh, NC', Followers=u'9614', Following=u'2554'),\n",
       " Row(Handle_User=u'_____fumi', Name=u'Fumi Omori', Bio=u'designer', Country=u'US', Location=u'Manhattan, NY', Followers=u'25', Following=u'106'),\n",
       " Row(Handle_User=u'___AnaAlicia', Name=u'Ana Alicia Gomez', Bio=u'#FFGF \\ufffd make poi feel some type of way', Country=u'US', Location=u'Lansing, MI', Followers=u'236', Following=u'245'),\n",
       " Row(Handle_User=u'___antony', Name=u'Tonik Salazar', Bio=u'Dj Overange with Billionaire Boyz Club', Country=u'MX', Location=u'Iztacalco, Distrito Federal', Followers=u'143', Following=u'143'),\n",
       " Row(Handle_User=u'___emelyyy', Name=u'E M E L \\ufffd', Bio=u'21| God First| California |Chris ??|Catracha', Country=u'US', Location=u'Los Angeles, CA', Followers=u'27', Following=u'111')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load exam1_users.csv file\n",
    "#its loaded in form of a distrubuted pyspark dataframe\n",
    "#for parallel computation\n",
    "#loading everything in stringtype to handle data variations like null value\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType\n",
    "schema_users = StructType([\n",
    "    StructField(\"Handle_User\", StringType()),\n",
    "    StructField(\"Name\", StringType()),\n",
    "    StructField(\"Bio\", StringType()),\n",
    "    StructField(\"Country\", StringType()),\n",
    "    StructField(\"Location\", StringType()),\n",
    "    StructField(\"Followers\", StringType()),\n",
    "    StructField(\"Following\", StringType())\n",
    "])\n",
    "users_df = (sqlContext.read\n",
    "    .schema(schema_users)\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\n",
    "    .csv(\"exam1_users.csv\"))\n",
    "users_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Handle=u'BillSchulhoff', Tweets=u'Wind 3.2 mph NNE. Barometer 30.20 in, Rising slowly. Temperature 49.3 \\ufffdF. Rain today 0.00 in. Humidity 32%', Favs=None, RTs=None, Latitude=u'40.76027778', Longitude=u'-72.95472221999999'),\n",
       " Row(Handle=u'danipolis', Tweets=u'Pausa pro caf\\ufffd antes de embarcar no pr\\ufffdximo v\\ufffdo. #trippolisontheroad #danipolisviaja Pause for\\ufffd https://t.co/PhcJ4oYktP', Favs=None, RTs=None, Latitude=u'32.89834949', Longitude=u'-97.03919589'),\n",
       " Row(Handle=u'KJacobs27', Tweets=u'Good. Morning. #morning #Saturday #diner #VT #breakfast #nucorpsofcadetsring #ring #college\\ufffd https://t.co/dBZ7dbwX6f', Favs=None, RTs=None, Latitude=u'44.199476', Longitude=u'-72.50417299999999'),\n",
       " Row(Handle=u'stncurtis', Tweets=u'@gratefuldead recordstoredayus ?????? @ TOMS MUSIC TRADE https://t.co/CURRmn6iJo', Favs=None, RTs=None, Latitude=u'39.901474', Longitude=u'-76.60681700000001'),\n",
       " Row(Handle=u'wi_borzo', Tweets=u'Egg in a muffin!!! (@ Rocket Baby Bakery - @rocketbabybaker in Wauwatosa, WI) https://t.co/mwfhrcxtRp', Favs=None, RTs=None, Latitude=u'43.06084924', Longitude=u'-87.99830888')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load exam1_tweets.csv file\n",
    "#its loaded in form of a distrubuted pyspark dataframe\n",
    "#for parallel computation\n",
    "schema_tweets = StructType([\n",
    "    StructField(\"Handle\", StringType()),\n",
    "    StructField(\"Tweets\", StringType()),\n",
    "    StructField(\"Favs\", StringType()),\n",
    "    StructField(\"RTs\", StringType()),\n",
    "    StructField(\"Latitude\", StringType()),\n",
    "    StructField(\"Longitude\", StringType())\n",
    "])\n",
    "tweets_df = (sqlContext.read\n",
    "    .schema(schema_tweets)\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\n",
    "    .csv(\"exam1_tweets.csv\"))\n",
    "tweets_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Handle=u'BillSchulhoff', Tweets=u'Wind 3.2 mph NNE. Barometer 30.20 in, Rising slowly. Temperature 49.3 \\ufffdF. Rain today 0.00 in. Humidity 32%', Favs=None, RTs=None, Latitude=u'40.76027778', Longitude=u'-72.95472221999999', tweets_len_count=18),\n",
       " Row(Handle=u'danipolis', Tweets=u'Pausa pro caf\\ufffd antes de embarcar no pr\\ufffdximo v\\ufffdo. #trippolisontheroad #danipolisviaja Pause for\\ufffd https://t.co/PhcJ4oYktP', Favs=None, RTs=None, Latitude=u'32.89834949', Longitude=u'-97.03919589', tweets_len_count=14),\n",
       " Row(Handle=u'KJacobs27', Tweets=u'Good. Morning. #morning #Saturday #diner #VT #breakfast #nucorpsofcadetsring #ring #college\\ufffd https://t.co/dBZ7dbwX6f', Favs=None, RTs=None, Latitude=u'44.199476', Longitude=u'-72.50417299999999', tweets_len_count=11),\n",
       " Row(Handle=u'stncurtis', Tweets=u'@gratefuldead recordstoredayus ?????? @ TOMS MUSIC TRADE https://t.co/CURRmn6iJo', Favs=None, RTs=None, Latitude=u'39.901474', Longitude=u'-76.60681700000001', tweets_len_count=8),\n",
       " Row(Handle=u'wi_borzo', Tweets=u'Egg in a muffin!!! (@ Rocket Baby Bakery - @rocketbabybaker in Wauwatosa, WI) https://t.co/mwfhrcxtRp', Favs=None, RTs=None, Latitude=u'43.06084924', Longitude=u'-87.99830888', tweets_len_count=14)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a column to add length of tweets\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "slen = udf(lambda s: len(s.split()), IntegerType())\n",
    "tweets_df = tweets_df.withColumn(\"tweets_len_count\", slen(tweets_df.Tweets))\n",
    "tweets_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Handle=u'511NY', total_len=23528), Row(Handle=u'CVSHealthJobs', total_len=10265), Row(Handle=u'WDCareers', total_len=9844), Row(Handle=u'SONICjobs', total_len=7178), Row(Handle=u'WorkWithSHC', total_len=7012), Row(Handle=u'CompassJobBoard', total_len=6947), Row(Handle=u'FavoriteJobs', total_len=6771), Row(Handle=u'VSIcareers', total_len=6680), Row(Handle=u'SpeedwayJobs', total_len=5441), Row(Handle=u'VirtualJukebox', total_len=5144), Row(Handle=u'Sunrise_Careers', total_len=5120), Row(Handle=u'Kindred_Jobs', total_len=4955), Row(Handle=u'AccountableHS', total_len=4710), Row(Handle=u'FTGiGSJobs', total_len=4598), Row(Handle=u'MercyJobs', total_len=4511), Row(Handle=u'dine_here', total_len=4419), Row(Handle=u'tmj_ca_hrta', total_len=4411), Row(Handle=u'JobsatVA', total_len=4372), Row(Handle=u'NevadaCountyWX', total_len=4335), Row(Handle=u'HMSHostCareers', total_len=4034), Row(Handle=u'workatavalonbay', total_len=3696), Row(Handle=u'TotalTrafficDFW', total_len=3657), Row(Handle=u'JoinBAYADA', total_len=3642), Row(Handle=u'dressbarnjobs', total_len=3420), Row(Handle=u'tmj_tx_retail', total_len=3362), Row(Handle=u'Diabetes_Newzz', total_len=3360), Row(Handle=u'tmj_ca_retail', total_len=3312), Row(Handle=u'DestinationJobs', total_len=3266), Row(Handle=u'ebbtidebot', total_len=3189), Row(Handle=u'tmj_in_retail', total_len=3116), Row(Handle=u'SutterJobs', total_len=3004), Row(Handle=u'tmj_lax_legal', total_len=2999), Row(Handle=u'tmj_lax_hrta', total_len=2994), Row(Handle=u'tmj_lax_health', total_len=2993), Row(Handle=u'LanguageRecruit', total_len=2977), Row(Handle=u'DGCareers', total_len=2955), Row(Handle=u'TravelNurseWork', total_len=2929), Row(Handle=u'CintasCareers', total_len=2805), Row(Handle=u'regionsjobs', total_len=2724), Row(Handle=u'tmj_hou_health', total_len=2679)]\n"
     ]
    }
   ],
   "source": [
    "#doing an aggregation to sum up all the length of tweets per user\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import desc\n",
    "tweets_df_shortlist = tweets_df.groupBy('Handle').agg(sum('tweets_len_count').alias('total_len')).sort(desc('total_len'))\n",
    "print tweets_df_shortlist.take(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Handle_User=u'511NY', Name=u'511 New York', Bio=u'Traffic & transit updates for all of New York State provided by New York State 511. Visit the website for more feeds.', Country=u'US', Location=u'Trenton, NJ', Followers=u'1285', Following=u'206', Handle=u'511NY', total_len=23528),\n",
       " Row(Handle_User=u'CVSHealthJobs', Name=u'CVS Health Jobs', Bio=u\"We're #hiring! Explore a variety of open positions that offer a challenging & rewarding #career. Follow us to uncover #job opportunities at @CVSHealth.\", Country=u'US', Location=u'Texas, USA', Followers=u'628', Following=u'3', Handle=u'CVSHealthJobs', total_len=10265),\n",
       " Row(Handle_User=u'WDCareers', Name=u'Winn-Dixie Careers', Bio=u\"Join our WINN-ing team and help make the lives of our customers and fellow associates FUN! Winn-Dixie is one of the nation's largest food retailers.\", Country=u'US', Location=u'Edgewood, FL', Followers=u'527', Following=u'252', Handle=u'WDCareers', total_len=9844),\n",
       " Row(Handle_User=u'SONICjobs', Name=u'SONIC Jobs', Bio=u'Check out our open positions to learn how you can WORK YOUR SPIRIT at #SONIC!', Country=u'US', Location=u'Texas, USA', Followers=u'268', Following=u'39', Handle=u'SONICjobs', total_len=7178),\n",
       " Row(Handle_User=u'WorkWithSHC', Name=u'SHC Careers', Bio=u'Work for the Best! Whether you want to work across town or across the country, we have thousands of great health care jobs available at top facilities.', Country=u'US', Location=u'Covington, GA', Followers=u'926', Following=u'1', Handle=u'WorkWithSHC', total_len=7012)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing a join with users_df on handle\n",
    "df_final = users_df.join(tweets_df_shortlist, users_df.Handle_User == tweets_df_shortlist.Handle).sort(desc('total_len'))\n",
    "df_final.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the column names required\n",
    "#descending sorting to get the top n users\n",
    "#repartitioning into 1 file to save it to hdfs\n",
    "#if multiple systems connected partitioning can be made n\n",
    "df_final.select('Handle', 'Name', 'Bio', 'Followers', 'Following', 'total_len').sort(desc('total_len')).limit(n).repartition(1).write.csv(\"final.hdfs\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
